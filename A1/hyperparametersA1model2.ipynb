{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning Methodology in Task A1 (Model 2)\n",
    "\n",
    "This Jupyter Notebook shows the methodology used in task A1 to pick the best parameters for model 2. This model uses Local Binary Patterns (LBP) as features for a Support Vector Machine (SVM).\n",
    "\n",
    "In order to observe the impact of the models hyper-parameters, Grid Search Cross-Validation was performed with a variety of possible parameters. This method undertakes an exhaustive search over given parameter settings, as to find the combination of parameters which will perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import glob, os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & pre-processing data\n",
    "\n",
    "The steps taken when importing & pre-processing the data are the same as the ones performed in the final model in A1.py, and described in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainA1LBP():\n",
    "    '''\n",
    "    Extracts LBP histograms for each picture\n",
    "    Performs train/test spliting (90% train, 10% test)\n",
    "    Implements dimensionality reduction by scaling and performing PCA\n",
    "    \n",
    "    Returns:\n",
    "        - pca_train : Train dataset of LBP after PCA\n",
    "        - pca_test : Test dataset of LBP after PCA\n",
    "        - lbs_train : Labels of training dataset\n",
    "        - lbs_test : Labels of testing dataset\n",
    "    '''\n",
    "\n",
    "    # Extracting LBP histograms\n",
    "    imgs, lbs = extract_lbp()\n",
    "\n",
    "    # Splitting dataset into 90% train and 10% test\n",
    "    data_train, data_test, lbs_train, lbs_test = train_test_split(imgs, lbs, test_size=0.1)\n",
    "\n",
    "    # Applying dimensionality reduction to dataset\n",
    "    pca_train, pca_test = dimensionality_reductionLBP(data_train, data_test)\n",
    "\n",
    "    return pca_train, pca_test, lbs_train, lbs_test\n",
    "\n",
    "def extract_lbp():\n",
    "    '''\n",
    "    Converts images to grayscale for LBP to be applied\n",
    "    Computes LBP for each picture\n",
    "    Implements histogram of LBP\n",
    "\n",
    "    Returns:\n",
    "        - hist_lbp : Dataset of images after LBP histogram computation\n",
    "        - lbs : Labels of entire dataset\n",
    "    '''\n",
    "\n",
    "    # Obtaining grayscale images and respective labels\n",
    "    imgs, lbs = grayscale()\n",
    "\n",
    "    # Defining parameters for LBP computation\n",
    "    # radius : Defines radius of circle of neighours\n",
    "    # numPoints : Defines number of neighbours to be used in LBP\n",
    "    numImgs = len(imgs)\n",
    "    radius = 2\n",
    "    numPoints = 30\n",
    "    hist_lbp = np.ones((numImgs, numPoints+2))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        img = local_binary_pattern(img, numPoints, radius, \"uniform\")\n",
    "        (hist, _) = np.histogram(img.ravel(), bins=np.arange(0, numPoints + 3),range=(0, numPoints + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= hist.sum()\n",
    "        hist_lbp[i,:] = hist\n",
    "\n",
    "    return hist_lbp, lbs\n",
    "\n",
    "def grayscale():\n",
    "    '''\n",
    "    Converts all images into grayscale\n",
    "\n",
    "    Returns:\n",
    "        - imgs : Entire dataset of grayscale images\n",
    "        - labels : Labels of entire dataset\n",
    "    '''\n",
    "\n",
    "    # Extracting labels\n",
    "    basedir = '../Datasets/dataset/A/'\n",
    "    labels_file = open(os.path.join(basedir,'labels.csv'), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    gender_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}\n",
    "\n",
    "    imgs = []\n",
    "    all_labels = []\n",
    "    dirA1 = os.path.join(basedir,'img/')\n",
    "\n",
    "    # Iterating over each image and converting it to grayscale\n",
    "    for filename in sorted(os.listdir(dirA1), key = lambda x : int(x[:-4])):\n",
    "\n",
    "        img = np.array(Image.open(os.path.join(dirA1,filename)).convert('L'))\n",
    "        imgs.append(img)\n",
    "        all_labels.append(gender_labels[filename[:-4]])\n",
    "    \n",
    "    labels = np.array(all_labels)\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "def dimensionality_reductionLBP(train_data, test_data):\n",
    "    '''\n",
    "    Scales train and test datasets\n",
    "    Implements Principal Component Analysis (PCA) on both datasets\n",
    "\n",
    "    Keyword arguments:\n",
    "        - train_data : Raw train dataset of LBP\n",
    "        - test_data : Raw test dataset of LBP\n",
    "\n",
    "    Returns:\n",
    "        - train_pca : Train dataset of LBP after PCA\n",
    "        - test_pca : Train dataset of LBP after PCA\n",
    "    '''\n",
    "\n",
    "    # Scaling datasets\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    # Applying PCA to datasets\n",
    "    # 'mle' algorithm not used since n_components > n_features\n",
    "    pca = PCA(n_components = 0.8, svd_solver = 'full')\n",
    "    pca.fit(train_data)\n",
    "    train_pca = pca.transform(train_data)\n",
    "    test_pca = pca.transform(test_data)\n",
    "\n",
    "    return train_pca, test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, lbs_train, lbs_test = mainA1LBP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross-Validation with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distribution to perform the search on\n",
    "param_dist = { \n",
    "    # Kernel type to be used in the algorithm\n",
    "    'kernel': ('linear', 'rbf'),   \n",
    "\n",
    "    # Regularization parameter\n",
    "    'C': [0.1,0.3,1,3,10,30],\n",
    "\n",
    "    # Kernel coefficient if kernel is 'rbf'\n",
    "    'gamma': ['scale',0.001,0.01,0.1,0.3,1],\n",
    "\n",
    "    # Specifying the seed for random distribution of data\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    '''\n",
    "    Helper function to report best scores for model\n",
    "    '''\n",
    "    \n",
    "    for i in range(1, n_top + 1): \n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 5.78 minutes for 72 candidate parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.631 (std: 0.017)\n",
      "Parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.628 (std: 0.016)\n",
      "Parameters: {'C': 3, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.628 (std: 0.016)\n",
      "Parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)\n",
    "start = time.time()\n",
    "grid_search.fit(data_train, lbs_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f minutes for %d candidate parameter settings.\"\n",
    "    % (round((time.time() - start)/60,2), len(grid_search.cv_results_['params'])))\n",
    "print(\"\")\n",
    "\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross-Validation without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainA1LBPSansPCA():\n",
    "    '''\n",
    "    Extracts LBP histograms for each picture\n",
    "    Performs train/test spliting (90% train, 10% test)\n",
    "    Implements dimensionality reduction by scaling and performing PCA\n",
    "    \n",
    "    Returns:\n",
    "        - pca_train : Train dataset of LBP after PCA\n",
    "        - pca_test : Test dataset of LBP after PCA\n",
    "        - lbs_train : Labels of training dataset\n",
    "        - lbs_test : Labels of testing dataset\n",
    "    '''\n",
    "\n",
    "    # Extracting LBP histograms\n",
    "    imgs, lbs = extract_lbp()\n",
    "\n",
    "    # Splitting dataset into 90% train and 10% test\n",
    "    data_train, data_test, lbs_train, lbs_test = train_test_split(imgs, lbs, test_size=0.1)\n",
    "\n",
    "    return data_train, data_test, lbs_train, lbs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, lbs_train, lbs_test = mainA1LBPSansPCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 3.15 minutes for 72 candidate parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.728 (std: 0.007)\n",
      "Parameters: {'C': 30, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.723 (std: 0.014)\n",
      "Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.702 (std: 0.017)\n",
      "Parameters: {'C': 3, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)\n",
    "start = time.time()\n",
    "grid_search.fit(data_train, lbs_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f minutes for %d candidate parameter settings.\"\n",
    "    % (round((time.time() - start)/60,2), len(grid_search.cv_results_['params'])))\n",
    "print(\"\")\n",
    "\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Observing the results of Grid Search Cross-Validation with and without PCA, it is possible to conclude that the SVM model performs (and generalizes) best when PCA is implemented, as the mean validation score for that model 74.4 ± 1.5 %, whereas for the non-PCA model it is 74.1 ± 1.4%. \n",
    "As such, the model with PCA will be used in the main code.\n",
    "\n",
    "Furthermore, the parameters of the model with the highest rank in the PCA model will be used as to get the best performance possible. They are:\n",
    "* Regularization parameter (C) : 10\n",
    "* Gamma : 0.01\n",
    "* Kernel Function : Radial basis function (RBF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
