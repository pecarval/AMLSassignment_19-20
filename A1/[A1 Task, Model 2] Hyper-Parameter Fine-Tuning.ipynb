{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning Methodology in Task A1 (Model 2)\n",
    "\n",
    "This Jupyter Notebook shows the methodology used in task A1 to pick the best parameters for model 2. This model uses Local Binary Patterns (LBP) as features for a Support Vector Machine (SVM).\n",
    "\n",
    "In order to observe the impact of the models hyper-parameters, Grid Search Cross-Validation was performed with a variety of possible parameters. This method undertakes an exhaustive search over given parameter settings, as to find the combination of parameters which will perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob, os\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import image\n",
    "import numpy as np\n",
    "import time\n",
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & pre-processing data\n",
    "\n",
    "The steps taken when importing & pre-processing the data are the same as the ones performed in the final model in A1.py, and described in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainA1LBP():\n",
    "    imgs, lbs = extract_lbp()\n",
    "    data_train, data_test, lbs_tr, lbs_te = train_test_split(imgs, lbs, test_size=0.2)\n",
    "    pca_train, pca_test = dimensionality_reductionLBP(data_train, data_test)\n",
    "    return pca_train, pca_test, lbs_tr, lbs_te\n",
    "    #return data_train, data_test, lbs_tr, lbs_te\n",
    "\n",
    "def extract_lbp():\n",
    "    imgs, lbs = grayscale()\n",
    "\n",
    "    numImgs = len(imgs)\n",
    "    radius = 8\n",
    "    numPoints = 24\n",
    "    hist_lbp = np.ones((numImgs, numPoints+2))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        img = local_binary_pattern(img, numPoints, radius, \"uniform\")\n",
    "        (hist, _) = np.histogram(img.ravel(), bins=np.arange(0, numPoints + 3),range=(0, numPoints + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= hist.sum()\n",
    "        hist_lbp[i,:] = hist\n",
    "\n",
    "    return hist_lbp, lbs\n",
    "\n",
    "def dimensionality_reductionLBP(train_dataset, test_dataset):\n",
    "    '''\n",
    "    Scales the data and performs Principal Component \n",
    "    Analysis (PCA) on a given dataset\n",
    "    '''\n",
    "\n",
    "    print(\"Dimensionality reduction started!\")\n",
    "    time0 = time.time()\n",
    "    print(\"PRE-PCA TRAIN SHAPE: \", train_dataset.shape)\n",
    "    print(\"PRE-PCA TEST SHAPE: \", test_dataset.shape)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_dataset)\n",
    "    \n",
    "    train_dataset = scaler.transform(train_dataset)\n",
    "    test_dataset = scaler.transform(test_dataset)\n",
    "\n",
    "    pca = PCA(n_components = 'mle', svd_solver = 'full')\n",
    "\n",
    "    pca.fit(train_dataset)\n",
    "    train_dataset = pca.transform(train_dataset)\n",
    "    test_dataset = pca.transform(test_dataset)\n",
    "\n",
    "    time1 = time.time()\n",
    "    print(\"PCA finished, it took: \", (time1-time0)/60, \" min\")\n",
    "    \n",
    "    print(\"Post-PCA TRAIN SHAPE: \", train_dataset.shape)\n",
    "    print(\"Post-PCA TEST SHAPE: \", test_dataset.shape)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def grayscale():\n",
    "    '''\n",
    "    Converts all images into grayscale\n",
    "    '''\n",
    "\n",
    "    basedir = '../Datasets/dataset/Original Datasets/celeba/'\n",
    "    labels_file = open(os.path.join(basedir,'labels.csv'), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    gender_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}\n",
    "\n",
    "    imgs = []\n",
    "    all_labels = []\n",
    "\n",
    "    dirA1 = os.path.join(basedir,'img/')\n",
    "\n",
    "    # Iterating over images in a sorted order\n",
    "    for filename in sorted(os.listdir(dirA1), key = lambda x : int(x[:-4])):\n",
    "\n",
    "        img = np.array(Image.open(os.path.join(dirA1,filename)).convert('L'))\n",
    "        imgs.append(img)\n",
    "        all_labels.append(gender_labels[filename[:-4]])\n",
    "    \n",
    "    labels = np.array(all_labels)\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality reduction started!\n",
      "PRE-PCA TRAIN SHAPE:  (4000, 26)\n",
      "PRE-PCA TEST SHAPE:  (1000, 26)\n",
      "PCA finished, it took:  0.00043870210647583007  min\n",
      "Post-PCA TRAIN SHAPE:  (4000, 25)\n",
      "Post-PCA TEST SHAPE:  (1000, 25)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, lbs_train, lbs_test = mainA1LBP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross-Validation Implementation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distribution to perform the search on\n",
    "param_dist = { \n",
    "    # Kernel type to be used in the algorithm\n",
    "    'kernel': ('linear', 'rbf'),   \n",
    "\n",
    "    # Regularization parameter\n",
    "    'C': [0.1,0.3,1,3,10,30],\n",
    "\n",
    "    # Kernel coefficient if kernel is 'rbf'\n",
    "    'gamma': ['scale',0.001,0.01,0.1,0.3,1],\n",
    "\n",
    "    # Specifying the seed for random distribution of data\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    '''\n",
    "    Helper function to report best scores for model\n",
    "    '''\n",
    "    \n",
    "    for i in range(1, n_top + 1): \n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 11.35 minutes for 72 candidate parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.643 (std: 0.015)\n",
      "Parameters: {'C': 30, 'gamma': 0.01, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.640 (std: 0.010)\n",
      "Parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.637 (std: 0.016)\n",
      "Parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)\n",
    "start = time.time()\n",
    "grid_search.fit(data_train, lbs_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f minutes for %d candidate parameter settings.\"\n",
    "    % ((time.time() - start)/60, len(grid_search.cv_results_['params'])))\n",
    "print(\"\")\n",
    "\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
