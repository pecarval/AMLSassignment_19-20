{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning Methodology in Task A1 (Model 1)\n",
    "\n",
    "This Jupyter Notebook shows the methodology used in task A1 to pick the best parameters for model 1. This model uses face landmarks (provided in lab 2) as features for a Support Vector Machine (SVM).\n",
    "\n",
    "In order to observe the impact of the models hyper-parameters, Grid Search Cross-Validation was performed with a variety of possible parameters. This method undertakes an exhaustive search over given parameter settings, as to find the combination of parameters which will perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob, os, sys\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import image\n",
    "import numpy as np\n",
    "import time\n",
    "sys.path.append('../Datasets/LandmarksFT/')\n",
    "import landmarksA1 as import_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & pre-processing data\n",
    "\n",
    "The steps taken when importing & pre-processing the data are the same as the ones performed in the final model in A1.py, and described in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainA1():\n",
    "    tr_data, tr_lbs, te_data, te_lbs = landmark_computation()\n",
    "    data_train = tr_data.reshape(tr_data.shape[0], tr_data.shape[1]*tr_data.shape[2])\n",
    "    data_test = te_data.reshape(te_data.shape[0], te_data.shape[1]*te_data.shape[2])\n",
    "    \n",
    "    pca_train, pca_test = dimensionality_reduction(data_train, data_test)\n",
    "    return pca_train, pca_test, tr_lbs, te_lbs\n",
    "\n",
    "def landmark_computation():\n",
    "    imgs, lbs = import_data.extract_features_labels()\n",
    "    tr_data, te_data, tr_lbs, te_lbs = train_test_split(imgs, lbs, test_size=0.2)\n",
    "    return tr_data, tr_lbs, te_data, te_lbs\n",
    "\n",
    "def dimensionality_reduction(train_dataset, test_dataset):\n",
    "    '''\n",
    "    Scales the data and performs Principal Component \n",
    "    Analysis (PCA) on a given dataset\n",
    "    '''\n",
    "\n",
    "    print(\"Dimensionality reduction started!\")\n",
    "    time0 = time.time()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_dataset)\n",
    "    train_dataset = scaler.transform(train_dataset)\n",
    "    test_dataset = scaler.transform(test_dataset)\n",
    "\n",
    "    pca = PCA(n_components = 'mle', svd_solver = 'full')\n",
    "\n",
    "    pca.fit(train_dataset)\n",
    "    train_dataset = pca.transform(train_dataset)\n",
    "    test_dataset = pca.transform(test_dataset)\n",
    "\n",
    "    time1 = time.time()\n",
    "    print(\"Dimensionality reduction finished, it took: \", (time1-time0)/60, \" min\")\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality reduction started!\n",
      "Dimensionality reduction finished, it took:  0.01963853438695272  min\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, lbs_train, lbs_test = mainA1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross-Validation with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distribution to perform the search on\n",
    "param_dist = { \n",
    "    # Kernel type to be used in the algorithm\n",
    "    'kernel': ('linear', 'rbf'),   \n",
    "\n",
    "    # Regularization parameter\n",
    "    'C': [0.1,0.3,1,3,10,30],\n",
    "\n",
    "    # Kernel coefficient if kernel is 'rbf'\n",
    "    'gamma': ['scale',0.001,0.01,0.1,0.3,1],\n",
    "\n",
    "    # Specifying the seed for random distribution of data\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    '''\n",
    "    Helper function to report best scores for model\n",
    "    '''\n",
    "    \n",
    "    for i in range(1, n_top + 1): \n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 1491.22 seconds for 72 candidate parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.927 (std: 0.004)\n",
      "Parameters: {'C': 30, 'gamma': 0.001, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.922 (std: 0.005)\n",
      "Parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.922 (std: 0.005)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.922 (std: 0.005)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.922 (std: 0.005)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.922 (std: 0.005)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.3, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.922 (std: 0.005)\n",
      "Parameters: {'C': 0.1, 'gamma': 1, 'kernel': 'linear', 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)\n",
    "start = time.time()\n",
    "grid_search.fit(data_train, lbs_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "    % (time.time() - start, len(grid_search.cv_results_['params'])))\n",
    "print(\"\")\n",
    "\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross-Validation without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainA1SansPCA():\n",
    "    tr_data, tr_lbs, te_data, te_lbs = landmark_computation()\n",
    "    data_train = tr_data.reshape(tr_data.shape[0], tr_data.shape[1]*tr_data.shape[2])\n",
    "    data_test = te_data.reshape(te_data.shape[0], te_data.shape[1]*te_data.shape[2])\n",
    "    return data_train, data_test, tr_lbs, te_lbs\n",
    "\n",
    "data_train, data_test, lbs_train, lbs_test = mainA1SansPCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 28283.61 seconds for 72 candidate parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.925 (std: 0.009)\n",
      "Parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.925 (std: 0.009)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.925 (std: 0.009)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.925 (std: 0.009)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.925 (std: 0.009)\n",
      "Parameters: {'C': 0.1, 'gamma': 0.3, 'kernel': 'linear', 'random_state': 42}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.925 (std: 0.009)\n",
      "Parameters: {'C': 0.1, 'gamma': 1, 'kernel': 'linear', 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)\n",
    "start = time.time()\n",
    "grid_search.fit(data_train, lbs_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "    % (time.time() - start, len(grid_search.cv_results_['params'])))\n",
    "print(\"\")\n",
    "\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Observing the results of Grid Search Cross-Validation with and without PCA, it is possible to conclude that the SVM model performs (and generalizes) best when PCA is implemented, as the mean validation score for that model 92.7 ± 0.4 %, whereas for the non-PCA model it is 92.5 ± 0.9 %. \n",
    "As such, the model with PCA will be used in the main code.\n",
    "\n",
    "Furthermore, the parameters of the model with the highest rank in the PCA model will be used as to get the best performance possible. They are:\n",
    "* Regularization parameter (C) : 30\n",
    "* Gamma : 0.001\n",
    "* Kernel Function : Radial basis function (RBF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
