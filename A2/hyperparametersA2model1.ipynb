{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning Methodology in Task A2 (Model 2)\n",
    "\n",
    "This Jupyter Notebook shows the methodology used in task B1 to pick the best parameters for model 2. This model uses face landmarks (provided in lab 2) as features for a Support Vector Machine (SVM).\n",
    "\n",
    "In order to observe the impact of the models hyper-parameters, Grid Search Cross-Validation was performed with a variety of possible parameters. This method undertakes an exhaustive search over given parameter settings, as to find the combination of parameters which will perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import numpy as np\n",
    "import os, sys, time\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "sys.path.append(\"../HelperFunctions/\")\n",
    "import landmarksA2 as landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & pre-processing data\n",
    "\n",
    "The steps taken when importing & pre-processing the data are the same as the ones performed in the final model in A1.py, and described in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainA2Landmarks():\n",
    "    '''\n",
    "    Extracts facial landmarks for each picture\n",
    "    Performs train/test spliting (90% train, 10% test)\n",
    "    Implements dimensionality reduction by scaling and performing PCA\n",
    "    \n",
    "    Returns:\n",
    "        - pca_train : Train dataset of facial landmarks after PCA\n",
    "        - pca_test : Test dataset of facial landmarks after PCA\n",
    "        - lbs_train : Labels of training dataset\n",
    "        - lbs_test : Labels of testing dataset\n",
    "    '''\n",
    "    \n",
    "    # Extracting facil landmarks\n",
    "    imgs, lbs = landmarks.extract_features_labels('../Datasets/dataset/A/original/')\n",
    "\n",
    "    # Splitting data into 90% train and 10% test\n",
    "    tr_data, te_data, lbs_train, lbs_test = train_test_split(imgs, lbs, test_size=0.1)\n",
    "    data_train = tr_data.reshape(tr_data.shape[0], tr_data.shape[1]*tr_data.shape[2])\n",
    "    data_test = te_data.reshape(te_data.shape[0], te_data.shape[1]*te_data.shape[2])\n",
    "\n",
    "    # Applying dimensionality reduction\n",
    "    pca_train, pca_test = dimensionality_reduction(data_train, data_test)\n",
    "\n",
    "    return pca_train, pca_test, lbs_train, lbs_test\n",
    " \n",
    "\n",
    "def dimensionality_reduction(train_data, test_data):\n",
    "    '''\n",
    "    Scales train and test datasets\n",
    "    Implements Principal Component Analysis (PCA) on both datasets\n",
    "\n",
    "    Keyword arguments:\n",
    "        - train_data : Raw train dataset of facial landmarks\n",
    "        - test_data : Raw test dataset of facial landmarks\n",
    "\n",
    "    Returns:\n",
    "        - train_pca : Train dataset of facial landmarks after PCA\n",
    "        - test_pca : Train dataset of facial landmarks after PCA\n",
    "    '''\n",
    "\n",
    "    # Scaling both datasets\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    # Applying PCA to both datasets\n",
    "    pca = PCA(n_components = 'mle', svd_solver = 'full')\n",
    "    pca.fit(train_data)\n",
    "    train_pca = pca.transform(train_data)\n",
    "    test_pca = pca.transform(test_data)\n",
    "\n",
    "    return train_pca, test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, lbs_train, lbs_test = mainA2Landmarks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross-Validation with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distribution to perform the search on\n",
    "param_dist = { \n",
    "    # Kernel type to be used in the algorithm\n",
    "    'kernel': ('linear', 'rbf'),   \n",
    "\n",
    "    # Regularization parameter\n",
    "    'C': [0.1,0.3,1,3,10,30],\n",
    "    #'C': [30,50,70,100,150,200],\n",
    "\n",
    "    # Kernel coefficient if kernel is 'rbf'\n",
    "    'gamma': ['scale',0.001,0.01,0.1,0.3,1],\n",
    "\n",
    "    # Specifying the seed for random distribution of data\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    '''\n",
    "    Helper function to report best scores for model\n",
    "    '''\n",
    "    \n",
    "    for i in range(1, n_top + 1): \n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 26.27 minutes for 72 candidate parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.898 (std: 0.006)\n",
      "Parameters: {'C': 3, 'gamma': 0.001, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.897 (std: 0.008)\n",
      "Parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.896 (std: 0.008)\n",
      "Parameters: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf', 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(clf, param_grid=param_dist, cv=5)\n",
    "start = time.time()\n",
    "grid_search.fit(data_train, lbs_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f minutes for %d candidate parameter settings.\"\n",
    "    % (round((time.time() - start)/60,2), len(grid_search.cv_results_['params'])))\n",
    "print(\"\")\n",
    "\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Observing the results of Grid Search Cross-Validation, it is possible to conclude that the SVM model performs best for the following parameters:\n",
    "* Regularization parameter (C) : 3\n",
    "* Gamma : 0.001\n",
    "* Kernel Function : Radial basis function (RBF)\n",
    "\n",
    "It should also be noted that Grid-Search CV without PCA was not implemented due to the large computational time hat it would involve, making it difficult to get results in a sensible timeframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
